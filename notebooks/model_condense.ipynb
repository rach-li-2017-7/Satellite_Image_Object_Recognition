{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zekiyan/Library/CloudStorage/GoogleDrive-zettayan@gmail.com/My Drive/Entrepreneur/Sundai/Projects/satellite_image_recognition/Satellite_Image_Object_Recognition\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from IPython.display import SVG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os, re, sys, random, shutil, cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam, Nadam\n",
    "from tensorflow.keras import applications, optimizers\n",
    "from tensorflow.keras.applications import InceptionResNetV2\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, CSVLogger, LearningRateScheduler\n",
    "from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, ZeroPadding2D, Dropout\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import utils\n",
    "from utils.utils import get_label_mappings, conv_block, decoder_block, build_inception_resnetv2_unet, rgb_to_onehot, onehot_to_rgb, dice_coef, filter_mask_colors, convert_to_transparent_png, mask_images, display_image_with_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Color Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2id, id2code, name2id, id2name = get_label_mappings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "model = build_inception_resnetv2_unet(input_shape = (512, 512, 3))\n",
    "model.compile(optimizer=Adam(lr = 0.0001), loss='categorical_crossentropy', metrics=[dice_coef, \"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"model/InceptionResNetV2-UNet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the image\n",
    "image_path = \"upload/SerenityCommunity1920By1280_v1_20241229.jpg\"\n",
    "\n",
    "# Load and preprocess the image\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_resized = cv2.resize(image, (512, 512))  # Resize to match model input size\n",
    "image_array = np.expand_dims(image_resized, axis=0) / 255.0  # Normalize and add batch dimension\n",
    "\n",
    "# Make prediction\n",
    "predicted_mask = model.predict(image_array)\n",
    "\n",
    "# Convert the predicted mask to RGB\n",
    "predicted_mask_rgb = onehot_to_rgb(predicted_mask[0], id2code)\n",
    "\n",
    "# Resize the predicted mask back to the original image dimensions\n",
    "original_height, original_width = image.shape[:2]\n",
    "predicted_mask_resized = cv2.resize(predicted_mask_rgb, (original_width, original_height), interpolation=cv2.INTER_NEAREST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the mask to keep only red (buildings) and blue (roads)\n",
    "colors_to_keep = [(255, 0, 0), (0, 0, 255)]\n",
    "filtered_mask = filter_mask_colors(predicted_mask_resized, colors_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered mask as transparent PNG\n",
    "convert_to_transparent_png(filtered_mask, 'output/filtered_mask.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and resize images\n",
    "target_size = (1280, 1920)\n",
    "original_image, filtered_mask = mask_images(\n",
    "    'upload/SerenityCommunity1920By1280_v1_20241229.jpg',\n",
    "    'output/filtered_mask.png',\n",
    "    target_size\n",
    ")\n",
    "\n",
    "# Save the overlaid result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(original_image)\n",
    "plt.imshow(filtered_mask, alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.savefig('output/result_origin.jpg', bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and resize images\n",
    "target_size = (1280, 1920)\n",
    "original_image, filtered_mask = mask_images(\n",
    "    'upload/SerenityCloseUp_v0_20241229.jpg',\n",
    "    'output/filtered_mask.png',\n",
    "    target_size\n",
    ")\n",
    "\n",
    "# Save the overlaid result\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.imshow(original_image)\n",
    "plt.imshow(filtered_mask, alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.savefig('output/result_cad.jpg', bbox_inches='tight', pad_inches=0)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
